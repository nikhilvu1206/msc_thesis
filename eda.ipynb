{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA file\n",
    "This file contains code for the important EDA plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import groupby\n",
    "#!conda remove scipy scikit-learn -y\n",
    "#!conda install scipy scikit-learn -y\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, f1_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "#%pip install mord\n",
    "from mord import LogisticAT\n",
    "from sklearn.metrics import classification_report, roc_auc_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.svm import SVC\n",
    "#%pip install panelsplit\n",
    "#from panelsplit import PanelSplit\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.utils.validation import indexable\n",
    "from itertools import chain\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "#%pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "#%pip install xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from scipy.stats import boxcox\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "#%pip install smogn\n",
    "from smogn import smoter\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, RocCurveDisplay\n",
    "#%pip install lifelines\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import k_fold_cross_validation\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.statistics import proportional_hazard_test\n",
    "#%pip install scikit-survival\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import pickle\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import pointbiserialr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/Nik/Documents/thesis/PEP data copy/Original Stata files/')\n",
    "path = \"./PEP_all data_long_20220316.dta\"\n",
    "path2 = \"./PEP_demographics_20220316.dta\"\n",
    "df = pd.read_stata(path)\n",
    "df2 = pd.read_stata(path2)\n",
    "df2 = df2.rename(columns={'StudyID': 'studyID'})\n",
    "df_merged = pd.merge(df, df2, on='studyID', how='left')\n",
    "df = df_merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining all useful columns\n",
    "disease_columns = ['heartattack', 'strokes', 'congestiveHD', 'carcinoma', 'hipfracture', 'chronicLung', 'Hypertension', 'arthritis', 'DM_TG']\n",
    "outcome_columns = ['N_adl4dis', 'N_IADL5', 'N_mob4dis']\n",
    "prob_b_columns = [col for col in df.columns if col.startswith('prob_b')]\n",
    "other_columns = ['SCESD_ge16', 'BMI_ge30']\n",
    "confounder_columns = ['BMI_3cp', 'age_fu', 'female', 'white', 'vis3cat', 'hear3cat', 'smoker_fu', 'mu3', 'rx_fu', 'hospstay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_disease(df, disease_columns, date_col):\n",
    "    \"\"\"\n",
    "    Impute NaN values in disease columns based on historical data for each studyID.\n",
    "    If a disease was ever reported (1) by an individual, subsequent NaNs are set to 1.\n",
    "    If a disease was never reported before a NaN, it is set to 0.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The dataframe containing the data.\n",
    "    disease_columns (list): List of columns that are disease indicators.\n",
    "    date_col (str): Column name for the date to ensure chronological order.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with NaNs in disease columns imputed based on historical data.\n",
    "    \"\"\"\n",
    "    # Sort the df chronologically by studyID and date\n",
    "    df = df.sort_values(by=['studyID', date_col])\n",
    "\n",
    "    # Process each participant separately\n",
    "    for studyID, group in df.groupby('studyID'):\n",
    "        # Iterate through each disease column\n",
    "        for disease in disease_columns:\n",
    "            # Find the first instance where the disease was reported\n",
    "            first_report_index = group[disease].first_valid_index()\n",
    "\n",
    "            # Create to identify rows before the first report\n",
    "            if first_report_index is not None:\n",
    "                before_first_report = group.index < first_report_index\n",
    "            else:\n",
    "                before_first_report = pd.Series(False, index=group.index)\n",
    "\n",
    "            # Apply cumulative max to forward-fill reported diseases; NaNs before the first report are unaffected\n",
    "            group[disease] = group[disease].cummax()\n",
    "\n",
    "            # Fill NaNs before the first reported instance with 0 (since disease was never reported)\n",
    "            group.loc[before_first_report, disease] = group.loc[before_first_report, disease].fillna(0)\n",
    "\n",
    "            # Backfill any remaining NaNs after the first report with 1 (assuming disease persists)\n",
    "            group[disease] = group[disease].fillna(1)\n",
    "\n",
    "        # Place the modified group back into the main DataFrame\n",
    "        df.loc[group.index, disease_columns] = group[disease_columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "# example\n",
    "df = impute_disease(df, disease_columns, 'intdate')\n",
    "\n",
    "# Check the results\n",
    "#print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prob_b_columns = [col for col in df.columns if col.startswith('prob_b')]\n",
    "df[prob_b_columns] = df[prob_b_columns].fillna(0)\n",
    "#df[disease_columns] = df[disease_columns].fillna(0)\n",
    "df[other_columns] = df[other_columns].fillna(0)\n",
    "df[outcome_columns] = df[outcome_columns].fillna(0)\n",
    "\n",
    " # here i impute tiny bits of data that were missing from symptom columns, i fill 0 bc\n",
    "# i assume that the symptoms were simply not reported\n",
    "\n",
    "df[prob_b_columns] = df[prob_b_columns].astype(int)\n",
    "df[disease_columns] = df[disease_columns].astype(int)\n",
    "df[other_columns] = df[other_columns].astype(int)\n",
    "df[outcome_columns] = df[outcome_columns].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions and additional prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure intdate is in datetime format\n",
    "df['intdate'] = pd.to_datetime(df['intdate'])\n",
    "\n",
    "# Sort by studyID and interview date\n",
    "df = df.sort_values(by=['studyID', 'intdate'])\n",
    "\n",
    "# Initialize the TR column\n",
    "df['TR'] = 0\n",
    "\n",
    "# Loop through each participant\n",
    "for study_id, group in df.groupby('studyID'):\n",
    "    previous_restrict = 0\n",
    "    tr_counter = 0\n",
    "    \n",
    "    for idx in group.index:\n",
    "        if group.loc[idx, 'restrict'] == 0:\n",
    "            tr_counter += 1  # Increase counter when restrict is 0\n",
    "        else:\n",
    "            df.loc[idx, 'TR'] = tr_counter  # Assign TR when restrict turns to 1\n",
    "            tr_counter = 0  # Reset counter\n",
    "    \n",
    "    # Handle the case where the last rows for a participant have restrict == 0\n",
    "    if tr_counter > 0 and group['restrict'].iloc[-1] == 0:\n",
    "        df.loc[group.index[-1], 'TR'] = tr_counter\n",
    "\n",
    "df = df[df['restrict'] == 1.0]\n",
    "\n",
    "# Fill any remaining 0s in TR to ensure all observations are accounted for\n",
    "df['TR'] = df['TR'].replace(0, np.nan).ffill().fillna(0).astype(int)\n",
    "\n",
    "# Plot the new TR distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['TR'].plot(kind='hist', bins=range(int(df['TR'].min()), int(df['TR'].max()) + 2), alpha=0.7, color='blue')\n",
    "plt.title('Distribution of TR')\n",
    "plt.xlabel('Time Since Last Report (months)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(range(int(df['TR'].min()), int(df['TR'].max()) + 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the frequency of each unique integer value of TR\n",
    "tr_counts = df['TR'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "tr_counts.plot(kind='bar', edgecolor='k', alpha=0.7)\n",
    "plt.title('Frequency of Each Integer Value of TR')\n",
    "plt.xlabel('TR')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Apply log transformation to TR\n",
    "df['TR_log'] = np.log1p(df['TR'])  # Use log1p to handle TR = 0\n",
    "\n",
    "# Plot the frequency of the log-transformed TR values\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(df['TR_log'], bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.title('Distribution of Log-Transformed TR')\n",
    "plt.xlabel('Log-Transformed TR')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TR_quantile'] = scaler.fit_transform(df[['TR']])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12,10))\n",
    "\n",
    "sns.histplot(df['TR'], ax=axes[0,0])\n",
    "axes[0,0].set_title('Original TR distribution')\n",
    "\n",
    "sns.histplot(df['TR_lag1'], ax=axes[0,1])\n",
    "axes[0,1].set_title('Original lagged TR distribution (lag=1)')\n",
    "\n",
    "sns.histplot(df['TR_quantile'], ax=axes[1,0])\n",
    "axes[1,0].set_title('Quantile-transformed TR distribution')\n",
    "\n",
    "sns.histplot(X[numeric_features], ax=axes[1,1])\n",
    "axes[1,1].set_title('Quantile-transformed lagged TR distribution (lag=1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure intdate is in datetime format\n",
    "df['intdate'] = pd.to_datetime(df['intdate'])\n",
    "\n",
    "# Sort by studyID and interview date\n",
    "df = df.sort_values(by=['studyID', 'intdate'])\n",
    "\n",
    "\n",
    "\n",
    "#### Map column names\n",
    "# List of detailed symptoms based on PEP study\n",
    "detailed_symptoms = [\n",
    "    \"pain or stiffness in your joints\",\n",
    "    \"pain or stiffness in your back\",\n",
    "    \"leg pain on walking\",\n",
    "    \"weakness of your arms or legs\",\n",
    "    \"swelling in your feet or ankles\",\n",
    "    \"been fatigued (no energy/very tired)\",\n",
    "    \"difficulty breathing or shortness of breath\",\n",
    "    \"chest pain or tightness\",\n",
    "    \"poor or decreased vision\",\n",
    "    \"been dizzy or unsteady on your feet\",\n",
    "    \"a fall or injury\",\n",
    "    \"been afraid of falling\",\n",
    "    \"cold or flu symptoms\",\n",
    "    \"difficulty with sleeping\",\n",
    "    \"nausea, vomiting, diarrhea, or other stomach (abdominal) problem\",\n",
    "    \"a problem with your memory or difficulty thinking\",\n",
    "    \"been depressed\",\n",
    "    \"been anxious or worried\",\n",
    "    \"frequent or painful urination\",\n",
    "    \"lost control of your urine and wet yourself\",\n",
    "    \"has a family member or friend become seriously ill or had an accident\",\n",
    "    \"experienced the death or loss of a family member or friend\",\n",
    "    \"a change in your medications\",\n",
    "    \"a problem with alcohol\"\n",
    "]\n",
    "\n",
    "# Starting column index for prob_b columns\n",
    "start_index = 3\n",
    "\n",
    "# Create a mapping of prob_b columns to the new symptom descriptions\n",
    "column_mapping = {f'prob_b{i}': symptom for i, symptom in zip(range(start_index, start_index + len(detailed_symptoms)), detailed_symptoms)}\n",
    "\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "prob_b_columns = list(column_mapping.values())\n",
    "\n",
    "# Outcome columns\n",
    "outcome_columns = ['N_adl4dis', 'N_IADL5', 'N_mob4dis']\n",
    "\n",
    "# Initialize delta columns and combined_change\n",
    "for col in outcome_columns:\n",
    "    df['delta_' + col] = df[col] - df.groupby('studyID')[col].shift()\n",
    "    df['change_' + col] = df['delta_' + col].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "df['combined_change'] = df[[f'change_{col}' for col in outcome_columns]].max(axis=1).astype(int)\n",
    "\n",
    "# Calculate cumulative time since last report for each individual\n",
    "df['cumulative_time'] = df.groupby('studyID')['TR'].cumsum()\n",
    "\n",
    "# Initialize actual time until change columns\n",
    "df['actual_time_until_change'] = np.nan\n",
    "\n",
    "# Calculate actual time until the change occurred\n",
    "for study_id, group in df.groupby('studyID'):\n",
    "    change_indices = group.index[group['combined_change'] == 1].tolist()\n",
    "    if change_indices:\n",
    "        change_time = 0\n",
    "        for idx in group.index:\n",
    "            if idx in change_indices:\n",
    "                df.loc[idx, 'actual_time_until_change'] = change_time\n",
    "                change_time = 0\n",
    "            change_time += group.loc[idx, 'TR']\n",
    "\n",
    "# Calculate final intbloc value for each studyID\n",
    "final_intbloc = df.groupby('studyID')['intbloc'].transform('max')\n",
    "\n",
    "# Fill NaNs in actual_time_until_change for entries where no change occurred with the final intbloc value\n",
    "df['actual_time_until_change'].fillna(final_intbloc, inplace=True)\n",
    "\n",
    "\n",
    "# Filter participants with at least three observations\n",
    "df = df.groupby('studyID').filter(lambda x: len(x) >= 2)\n",
    "\n",
    "# Print the number of participants left after filtering\n",
    "num_participants = df['studyID'].nunique()\n",
    "print(f\"Number of participants with at least two observations: {num_participants}\")\n",
    "\n",
    "\n",
    "# Calculate symptom frequency and duration per participant\n",
    "for col in prob_b_columns:\n",
    "    df[f'{col}_frequency'] = df.groupby('studyID')[col].cumsum()\n",
    "    df[f'{col}_duration'] = df.groupby('studyID')[col].transform(lambda x: (x != 0).astype(int).groupby((x == 0).astype(int).cumsum()).cumsum())\n",
    "\n",
    "# Calculate symptom onset for each symptom\n",
    "for col in prob_b_columns:\n",
    "    df[f'{col}_onset'] = df.groupby('studyID').apply(lambda x: (x[col] * x['TR']).cumsum() - (x[col] * x['TR']).cumsum().where(x[col] == 0).ffill().fillna(0)).values\n",
    "\n",
    "# Calculate symptom recurrence for each symptom\n",
    "for col in prob_b_columns:\n",
    "    df[f'{col}_recurrence'] = df.groupby('studyID')[col].transform(lambda x: (x.diff().fillna(0) == 1).cumsum())\n",
    "\n",
    "# Define the range for lagged features\n",
    "lag_range = range(1, 2)\n",
    "\n",
    "# Create lagged features for symptoms, TR, and temporal features\n",
    "for col in prob_b_columns + ['TR'] + outcome_columns:\n",
    "    for lag in lag_range:\n",
    "        df[f'{col}_lag{lag}'] = df.groupby('studyID')[col].shift(lag)\n",
    "\n",
    "# Create lagged features for additional temporal features\n",
    "for col in [f'{symptom}_recurrence' for symptom in prob_b_columns] + \\\n",
    "           [f'{symptom}_onset' for symptom in prob_b_columns] + \\\n",
    "           [f'{symptom}_frequency' for symptom in prob_b_columns] + \\\n",
    "           [f'{symptom}_duration' for symptom in prob_b_columns]:\n",
    "    for lag in lag_range:\n",
    "        df[f'{col}_lag{lag}'] = df.groupby('studyID')[col].shift(lag)\n",
    "\n",
    "# Drop rows with NaN values in any of the lagged feature columns\n",
    "lagged_columns = [f'{col}_lag{lag}' for col in prob_b_columns + outcome_columns for lag in lag_range]\n",
    "lagged_temporal_columns = [f'{col}_lag{lag}' for col in \n",
    "                           [f'{symptom}_recurrence' for symptom in prob_b_columns] +\n",
    "                           [f'{symptom}_onset' for symptom in prob_b_columns] +\n",
    "                           [f'{symptom}_frequency' for symptom in prob_b_columns] +\n",
    "                           [f'{symptom}_duration' for symptom in prob_b_columns]\n",
    "                           for lag in lag_range]\n",
    "df = df.dropna(subset=lagged_columns + lagged_temporal_columns)\n",
    "\n",
    "# Store the lagged feature names in a variable\n",
    "lagged_symptom_features = [f'{col}_lag{lag}' for col in prob_b_columns for lag in lag_range]\n",
    "lagged_outcomes = [f'{col}_lag{lag}' for col in outcome_columns for lag in lag_range]\n",
    "lagged_tr_features = [f'TR_lag{lag}' for lag in lag_range]\n",
    "lagged_temporal_features = [f'{symptom}_recurrence_lag{lag}' for symptom in prob_b_columns for lag in lag_range] + \\\n",
    "                           [f'{symptom}_onset_lag{lag}' for symptom in prob_b_columns for lag in lag_range] + \\\n",
    "                           [f'{symptom}_frequency_lag{lag}' for symptom in prob_b_columns for lag in lag_range] + \\\n",
    "                           [f'{symptom}_duration_lag{lag}' for symptom in prob_b_columns for lag in lag_range]\n",
    "\n",
    "# Calculate total number of symptoms (lagged)\n",
    "df['total_symptoms_lagged'] = df[lagged_symptom_features].sum(axis=1)\n",
    "\n",
    "# Additional feature engineering: Calculate magnitude of changes\n",
    "for col in outcome_columns:\n",
    "    df[f'magnitude_change_{col}'] = df[f'delta_' + col].abs()\n",
    "\n",
    "# Define feature sets\n",
    "binary_features = prob_b_columns\n",
    "numeric_features = [col for col in df.columns if col not in binary_features + ['actual_time_until_change', 'total_symptoms_lagged']]\n",
    "temporal_features = [f'{col}_frequency' for col in prob_b_columns] + \\\n",
    "                    [f'{col}_duration' for col in prob_b_columns] + \\\n",
    "                    [f'{col}_onset' for col in prob_b_columns] + \\\n",
    "                    [f'{col}_recurrence' for col in prob_b_columns]\n",
    "\n",
    "# Separate temporal features into lagged and current sets\n",
    "current_temporal_features = temporal_features\n",
    "\n",
    "# Final dataframe for modeling (do not dropna)\n",
    "df = df.reset_index(drop=True)\n",
    "# Check the structure of the dataframe\n",
    "df.isna().sum()\n",
    "\n",
    "# Plot the distribution of actual_time_until_change\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#df['actual_time_until_change'].dropna().plot(kind='hist', bins=range(int(df['actual_time_until_change'].min()), int(df['actual_time_until_change'].max()) + 2), alpha=0.7, color='blue')\n",
    "#plt.title('Distribution of Actual Time Until Change')\n",
    "#plt.xlabel('Time Until Change (months)')\n",
    "#plt.ylabel('Frequency')\n",
    "#plt.xticks(range(int(df['actual_time_until_change'].min()), int(df['actual_time_until_change'].max()) + 1))\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the number of observations for each participant\n",
    "participant_lengths = df.groupby('studyID').size()\n",
    "\n",
    "# Create a histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(participant_lengths, bins=50, edgecolor='black')\n",
    "plt.xlabel('Number of Observations')\n",
    "plt.ylabel('Number of Participants')\n",
    "plt.title('Distribution of Time Series Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing end of life data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the end-of-life threshold (last 5 months)\n",
    "df['end_of_life_threshold'] = df['ddate'] - pd.DateOffset(months=5)\n",
    "\n",
    "\n",
    "# Remove the last 5 months of life for each participant who has died (Died == 1)\n",
    "df_no_eol = df[~((df['Died'] == 1) & (df['intdate'] >= df['end_of_life_threshold']))]\n",
    "\n",
    "df = df_no_eol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the end-of-life threshold (last 5 months)\n",
    "df['end_of_life_threshold'] = df['ddate'] - pd.DateOffset(months=5)\n",
    "\n",
    "# Plot the distribution of TR before removing end-of-life period\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(df['TR'], bins=range(df['TR'].min(), df['TR'].max() + 1), edgecolor='k', alpha=0.7)\n",
    "plt.title('Distribution of TR before Removing End-of-Life Period')\n",
    "plt.xlabel('TR')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Remove the last 5 months of life for each participant who has died (Died == 1)\n",
    "df_no_eol = df[~((df['Died'] == 1) & (df['intdate'] >= df['end_of_life_threshold']))]\n",
    "\n",
    "# Plot the distribution of TR after removing end-of-life period\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(df_no_eol['TR'], bins=range(df_no_eol['TR'].min(), df_no_eol['TR'].max() + 1), edgecolor='k', alpha=0.7)\n",
    "plt.title('Distribution of TR after Removing End-of-Life Period')\n",
    "plt.xlabel('TR')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot for N_adl4dis\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(df['N_adl4dis'], color='blue', label='N_adl4dis')\n",
    "plt.title('Distribution of N_adl4dis')\n",
    "plt.xlabel('N_adl4dis')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for N_IADL5\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(df['N_IADL5'], color='green', label='N_IADL5')\n",
    "plt.title('Distribution of N_IADL5')\n",
    "plt.xlabel('N_IADL5')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for N_mob4dis\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(df['N_mob4dis'], color='red', label='N_mob4dis')\n",
    "plt.title('Distribution of N_mob4dis')\n",
    "plt.xlabel('N_mob4dis')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 8  \n",
    "n_cols = 3  \n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(12, 26))  \n",
    "axes = axes.flatten()  # Flatten the axes array for easier itera\n",
    "\n",
    "# Loop through the list of columns and create a bar plot for each\n",
    "for i, col in enumerate(prob_b_columns):\n",
    "    # Count the frequency of each category in the current column\n",
    "    value_counts = df[col].value_counts()\n",
    "\n",
    "    # Create bar plot\n",
    "    value_counts.plot(kind='bar', ax=axes[i], color=['blue', 'orange'])\n",
    "    axes[i].set_xticklabels(['No', 'Yes'], rotation=0)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_ylim([0, df.shape[0]]) \n",
    "\n",
    "plt.title('Frequencies of all 24 pre-specified symptoms')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "sns.histplot(df['actual_time_until_change'])\n",
    "plt.title('Distribution of TUC')\n",
    "plt.xlabel('TUC')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming df contains the relevant TR and lagged TR columns\n",
    "tr_columns = ['TR'] + ['TR_lag1']\n",
    "tr_df = df[tr_columns]\n",
    "\n",
    "# Calculate Spearman correlation\n",
    "tr_spearman_corr = tr_df.corr(method='spearman')\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(tr_spearman_corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Spearman Correlation Heatmap for TR and Lagged TR')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_tr_corr = df[lagged_symptom_features + ['TR']].corr() # pearson\n",
    "print(sym_tr_corr['TR'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df[prob_b_columns].corr()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Symptoms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_columns = ['heartattack', 'strokes', 'congestiveHD', 'carcinoma', 'hipfracture', 'chronicLung', 'Hypertension', 'arthritis', 'DM_TG']\n",
    "\n",
    "\n",
    "# Calculate the sum of prob_b_columns for each disease column\n",
    "result_df = pd.DataFrame(index=prob_b_columns)\n",
    "\n",
    "for disease in disease_columns:\n",
    "    # For each disease, calculate the prevalence of each prob_b_column\n",
    "    result_df[disease] = df[df[disease] == 1][prob_b_columns].sum() / df[disease].sum()\n",
    "\n",
    "# Step 2: Create a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(result_df, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Prevalence of prob_b_columns for each Disease')\n",
    "plt.ylabel('Symptom columns')\n",
    "plt.xlabel('Disease')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for total symptoms and other columns\n",
    "correlation_matrix = df[['total_symptoms'] + columns_to_analyze].corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix of Total Symptoms and Other Variables')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold the correlation results\n",
    "correlation_results = []\n",
    "\n",
    "# Calculate point-biserial correlation for binary symptoms and continuous TR\n",
    "for col in lagged_symptom_features:\n",
    "    corr, p_value = pointbiserialr(df[col], df['TR'])\n",
    "    correlation_results.append({'Symptom': col, 'Correlation': corr, 'P-Value': p_value})\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "correlation_df = pd.DataFrame(correlation_results)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Symptom', y='Correlation', data=correlation_df, palette='viridis')\n",
    "plt.title('Point-Biserial Correlation between Lagged Symptoms (lag=1) and TR')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spearman-rank test for lagged symptoms and ordinal outcomes\n",
    "\n",
    "spearman = df[lagged_symptom_features + outcome_columns].corr(method='spearman')\n",
    "\n",
    "corr_mat = spearman.loc[lagged_symptom_features, outcome_columns]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corr_mat, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Spearman Rank Correlations between lagged symptoms (lag=1) and outcome scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHI square test for lagged symptoms vs event observed\n",
    "chi2_results = {}\n",
    "\n",
    "for i in lagged_symptom_features:\n",
    "    cont_table = pd.crosstab(df[i], df['event_observed'])\n",
    "    chi2, p, dof, ex = chi2_contingency(cont_table)\n",
    "    chi2_results[i] = {'chi2': chi2, 'p-value': p, 'degrees of freedom': dof}\n",
    "\n",
    "chi2_p_values = {i: result['p-value'] for i, result in chi2_results.items()}\n",
    "\n",
    "chi2_df = pd.DataFrame.from_dict(chi2_p_values, orient='index', columns=['p-value'])\n",
    "chi2_df = chi2_df.sort_values(by='p-value')\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "chi2_df['p-value'].plot(kind='bar', color='skyblue')\n",
    "plt.axhline(y=0.05, color='r', label='Significance Threshold is 0.05')\n",
    "plt.title('Chi-Squared Test p-values for lagged symptom features (lag=1) versus Change in outcome observed')\n",
    "plt.xlabel('Lagged Symptoms')\n",
    "plt.ylabel('p-value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate Cramér's V\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "# Define the categorical columns\n",
    "categorical_columns = lagged_symptom_features + prob_b_columns\n",
    "\n",
    "# Calculate Cramér's V for each pair of categorical columns\n",
    "cramers_v_matrix = pd.DataFrame(index=categorical_columns, columns=categorical_columns)\n",
    "\n",
    "for col1 in categorical_columns:\n",
    "    for col2 in categorical_columns:\n",
    "        cramers_v_matrix.loc[col1, col2] = cramers_v(df[col1], df[col2])\n",
    "\n",
    "print(cramers_v_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cramers_v_matrix = cramers_v_matrix.astype(float)\n",
    "plt.figure(figsize=(20,16))\n",
    "sns.heatmap(cramers_v_matrix, annot=False, cmap='coolwarm', cbar={'label': \"Cramer's V\"})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure intdate is in datetime format\n",
    "df['intdate'] = pd.to_datetime(df['intdate'])\n",
    "\n",
    "# Outcome columns\n",
    "outcome_columns = ['N_adl4dis', 'N_IADL5', 'N_mob4dis']\n",
    "\n",
    "# Calculate differences and combined changes\n",
    "for col in outcome_columns:\n",
    "    df['diff_' + col] = df.groupby('studyID')[col].diff().fillna(0)\n",
    "\n",
    "# Create a combined change column\n",
    "df['combined_diff'] = df[[f'diff_{col}' for col in outcome_columns]].sum(axis=1)\n",
    "\n",
    "# Remove rows with NaN values in combined_diff (there shouldn't be any due to fillna(0))\n",
    "valid_data = df.dropna(subset=['combined_diff'])\n",
    "\n",
    "# Calculate frequency of each difference\n",
    "frequency = valid_data['combined_diff'].value_counts().sort_index()\n",
    "\n",
    "# Define the range for the x-axis\n",
    "min_diff = valid_data['combined_diff'].min()\n",
    "max_diff = valid_data['combined_diff'].max()\n",
    "\n",
    "# Plot the combined change frequency\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(frequency.index, frequency.values, color='blue')\n",
    "plt.title('Change Frequency for Combined Outcomes')\n",
    "plt.xlabel('Combined Change in Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(range(int(min_diff), int(max_diff) + 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Author: Nikhil Kashyap, MSc Information Studies: Data Science, UvA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
