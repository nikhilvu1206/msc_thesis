{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive modeling file\n",
    "This file contains code for all the predictive modeling tasks performed for the MSc thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import groupby\n",
    "#!conda remove scipy scikit-learn -y\n",
    "#!conda install scipy scikit-learn -y\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, f1_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "#%pip install mord\n",
    "from mord import LogisticAT\n",
    "from sklearn.metrics import classification_report, roc_auc_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.svm import SVC\n",
    "#%pip install panelsplit\n",
    "#from panelsplit import PanelSplit\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.utils.validation import indexable\n",
    "from itertools import chain\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "#%pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "#%pip install xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from scipy.stats import boxcox\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "#%pip install smogn\n",
    "from smogn import smoter\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, RocCurveDisplay\n",
    "#%pip install lifelines\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import k_fold_cross_validation\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.statistics import proportional_hazard_test\n",
    "#%pip install scikit-survival\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/Nik/Documents/thesis/PEP data copy/Original Stata files/')\n",
    "path = \"./PEP_all data_long_20220316.dta\"\n",
    "path2 = \"./PEP_demographics_20220316.dta\"\n",
    "df = pd.read_stata(path)\n",
    "df2 = pd.read_stata(path2)\n",
    "df2 = df2.rename(columns={'StudyID': 'studyID'})\n",
    "df_merged = pd.merge(df, df2, on='studyID', how='left')\n",
    "df = df_merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining all useful columns\n",
    "disease_columns = ['heartattack', 'strokes', 'congestiveHD', 'carcinoma', 'hipfracture', 'chronicLung', 'Hypertension', 'arthritis', 'DM_TG']\n",
    "outcome_columns = ['N_adl4dis', 'N_IADL5', 'N_mob4dis']\n",
    "prob_b_columns = [col for col in df.columns if col.startswith('prob_b')]\n",
    "other_columns = ['SCESD_ge16', 'BMI_ge30']\n",
    "confounder_columns = ['BMI_3cp', 'age_fu', 'female', 'white', 'vis3cat', 'hear3cat', 'smoker_fu', 'mu3', 'rx_fu', 'hospstay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_disease(df, disease_columns, date_col):\n",
    "    \"\"\"\n",
    "    Impute NaN values in disease columns based on historical data for each studyID.\n",
    "    If a disease was ever reported (1) by an individual, subsequent NaNs are set to 1.\n",
    "    If a disease was never reported before a NaN, it is set to 0.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The dataframe containing the data.\n",
    "    disease_columns (list): List of columns that are disease indicators.\n",
    "    date_col (str): Column name for the date to ensure chronological order.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with NaNs in disease columns imputed based on historical data.\n",
    "    \"\"\"\n",
    "    # Sort the df chronologically by studyID and date\n",
    "    df = df.sort_values(by=['studyID', date_col])\n",
    "\n",
    "    # Process each participant separately\n",
    "    for studyID, group in df.groupby('studyID'):\n",
    "        # Iterate through each disease column\n",
    "        for disease in disease_columns:\n",
    "            # Find the first instance where the disease was reported\n",
    "            first_report_index = group[disease].first_valid_index()\n",
    "\n",
    "            # Create to identify rows before the first report\n",
    "            if first_report_index is not None:\n",
    "                before_first_report = group.index < first_report_index\n",
    "            else:\n",
    "                before_first_report = pd.Series(False, index=group.index)\n",
    "\n",
    "            # Apply cumulative max to forward-fill reported diseases; NaNs before the first report are unaffected\n",
    "            group[disease] = group[disease].cummax()\n",
    "\n",
    "            # Fill NaNs before the first reported instance with 0 (since disease was never reported)\n",
    "            group.loc[before_first_report, disease] = group.loc[before_first_report, disease].fillna(0)\n",
    "\n",
    "            # Backfill any remaining NaNs after the first report with 1 (assuming disease persists)\n",
    "            group[disease] = group[disease].fillna(1)\n",
    "\n",
    "        # Place the modified group back into the main DataFrame\n",
    "        df.loc[group.index, disease_columns] = group[disease_columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "# example\n",
    "df = impute_disease(df, disease_columns, 'intdate')\n",
    "\n",
    "# Check the results\n",
    "#print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prob_b_columns = [col for col in df.columns if col.startswith('prob_b')]\n",
    "df[prob_b_columns] = df[prob_b_columns].fillna(0)\n",
    "#df[disease_columns] = df[disease_columns].fillna(0)\n",
    "df[other_columns] = df[other_columns].fillna(0)\n",
    "df[outcome_columns] = df[outcome_columns].fillna(0)\n",
    "\n",
    " # here i impute tiny bits of data that were missing from symptom columns, i fill 0 bc\n",
    "# i assume that the symptoms were simply not reported\n",
    "\n",
    "df[prob_b_columns] = df[prob_b_columns].astype(int)\n",
    "df[disease_columns] = df[disease_columns].astype(int)\n",
    "df[other_columns] = df[other_columns].astype(int)\n",
    "df[outcome_columns] = df[outcome_columns].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure intdate is in datetime format\n",
    "df['intdate'] = pd.to_datetime(df['intdate'])\n",
    "\n",
    "# Sort by studyID and interview date\n",
    "df = df.sort_values(by=['studyID', 'intdate'])\n",
    "\n",
    "\n",
    "\n",
    "#### Map column names\n",
    "# List of detailed symptoms based on PEP study\n",
    "detailed_symptoms = [\n",
    "    \"pain or stiffness in your joints\",\n",
    "    \"pain or stiffness in your back\",\n",
    "    \"leg pain on walking\",\n",
    "    \"weakness of your arms or legs\",\n",
    "    \"swelling in your feet or ankles\",\n",
    "    \"been fatigued (no energy/very tired)\",\n",
    "    \"difficulty breathing or shortness of breath\",\n",
    "    \"chest pain or tightness\",\n",
    "    \"poor or decreased vision\",\n",
    "    \"been dizzy or unsteady on your feet\",\n",
    "    \"a fall or injury\",\n",
    "    \"been afraid of falling\",\n",
    "    \"cold or flu symptoms\",\n",
    "    \"difficulty with sleeping\",\n",
    "    \"nausea, vomiting, diarrhea, or other stomach (abdominal) problem\",\n",
    "    \"a problem with your memory or difficulty thinking\",\n",
    "    \"been depressed\",\n",
    "    \"been anxious or worried\",\n",
    "    \"frequent or painful urination\",\n",
    "    \"lost control of your urine and wet yourself\",\n",
    "    \"has a family member or friend become seriously ill or had an accident\",\n",
    "    \"experienced the death or loss of a family member or friend\",\n",
    "    \"a change in your medications\",\n",
    "    \"a problem with alcohol\"\n",
    "]\n",
    "\n",
    "# Starting column index for prob_b columns\n",
    "start_index = 3\n",
    "\n",
    "# Create a mapping of prob_b columns to the new symptom descriptions\n",
    "column_mapping = {f'prob_b{i}': symptom for i, symptom in zip(range(start_index, start_index + len(detailed_symptoms)), detailed_symptoms)}\n",
    "\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "prob_b_columns = list(column_mapping.values())\n",
    "\n",
    "# Outcome columns\n",
    "outcome_columns = ['N_adl4dis', 'N_IADL5', 'N_mob4dis']\n",
    "\n",
    "# Initialize delta columns and combined_change\n",
    "for col in outcome_columns:\n",
    "    df['delta_' + col] = df[col] - df.groupby('studyID')[col].shift()\n",
    "    df['change_' + col] = df['delta_' + col].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "df['combined_change'] = df[[f'change_{col}' for col in outcome_columns]].max(axis=1).astype(int)\n",
    "\n",
    "# Calculate cumulative time since last report for each individual\n",
    "df['cumulative_time'] = df.groupby('studyID')['TR'].cumsum()\n",
    "\n",
    "# Initialize actual time until change columns\n",
    "df['actual_time_until_change'] = np.nan\n",
    "\n",
    "# Calculate actual time until the change occurred\n",
    "for study_id, group in df.groupby('studyID'):\n",
    "    change_indices = group.index[group['combined_change'] == 1].tolist()\n",
    "    if change_indices:\n",
    "        change_time = 0\n",
    "        for idx in group.index:\n",
    "            if idx in change_indices:\n",
    "                df.loc[idx, 'actual_time_until_change'] = change_time\n",
    "                change_time = 0\n",
    "            change_time += group.loc[idx, 'TR']\n",
    "\n",
    "# Calculate final intbloc value for each studyID\n",
    "final_intbloc = df.groupby('studyID')['intbloc'].transform('max')\n",
    "\n",
    "# Fill NaNs in actual_time_until_change for entries where no change occurred with the final intbloc value\n",
    "df['actual_time_until_change'].fillna(final_intbloc, inplace=True)\n",
    "\n",
    "\n",
    "# Filter participants with at least three observations\n",
    "df = df.groupby('studyID').filter(lambda x: len(x) >= 2)\n",
    "\n",
    "# Print the number of participants left after filtering\n",
    "num_participants = df['studyID'].nunique()\n",
    "print(f\"Number of participants with at least two observations: {num_participants}\")\n",
    "\n",
    "\n",
    "# Calculate symptom frequency and duration per participant\n",
    "for col in prob_b_columns:\n",
    "    df[f'{col}_frequency'] = df.groupby('studyID')[col].cumsum()\n",
    "    df[f'{col}_duration'] = df.groupby('studyID')[col].transform(lambda x: (x != 0).astype(int).groupby((x == 0).astype(int).cumsum()).cumsum())\n",
    "\n",
    "# Calculate symptom onset for each symptom\n",
    "for col in prob_b_columns:\n",
    "    df[f'{col}_onset'] = df.groupby('studyID').apply(lambda x: (x[col] * x['TR']).cumsum() - (x[col] * x['TR']).cumsum().where(x[col] == 0).ffill().fillna(0)).values\n",
    "\n",
    "# Calculate symptom recurrence for each symptom\n",
    "for col in prob_b_columns:\n",
    "    df[f'{col}_recurrence'] = df.groupby('studyID')[col].transform(lambda x: (x.diff().fillna(0) == 1).cumsum())\n",
    "\n",
    "# Define the range for lagged features\n",
    "lag_range = range(1, 2)\n",
    "\n",
    "# Create lagged features for symptoms, TR, and temporal features\n",
    "for col in prob_b_columns + ['TR'] + outcome_columns:\n",
    "    for lag in lag_range:\n",
    "        df[f'{col}_lag{lag}'] = df.groupby('studyID')[col].shift(lag)\n",
    "\n",
    "# Create lagged features for additional temporal features\n",
    "for col in [f'{symptom}_recurrence' for symptom in prob_b_columns] + \\\n",
    "           [f'{symptom}_onset' for symptom in prob_b_columns] + \\\n",
    "           [f'{symptom}_frequency' for symptom in prob_b_columns] + \\\n",
    "           [f'{symptom}_duration' for symptom in prob_b_columns]:\n",
    "    for lag in lag_range:\n",
    "        df[f'{col}_lag{lag}'] = df.groupby('studyID')[col].shift(lag)\n",
    "\n",
    "# Drop rows with NaN values in any of the lagged feature columns\n",
    "lagged_columns = [f'{col}_lag{lag}' for col in prob_b_columns + outcome_columns for lag in lag_range]\n",
    "lagged_temporal_columns = [f'{col}_lag{lag}' for col in \n",
    "                           [f'{symptom}_recurrence' for symptom in prob_b_columns] +\n",
    "                           [f'{symptom}_onset' for symptom in prob_b_columns] +\n",
    "                           [f'{symptom}_frequency' for symptom in prob_b_columns] +\n",
    "                           [f'{symptom}_duration' for symptom in prob_b_columns]\n",
    "                           for lag in lag_range]\n",
    "df = df.dropna(subset=lagged_columns + lagged_temporal_columns)\n",
    "\n",
    "# Store the lagged feature names in a variable\n",
    "lagged_symptom_features = [f'{col}_lag{lag}' for col in prob_b_columns for lag in lag_range]\n",
    "lagged_outcomes = [f'{col}_lag{lag}' for col in outcome_columns for lag in lag_range]\n",
    "lagged_tr_features = [f'TR_lag{lag}' for lag in lag_range]\n",
    "lagged_temporal_features = [f'{symptom}_recurrence_lag{lag}' for symptom in prob_b_columns for lag in lag_range] + \\\n",
    "                           [f'{symptom}_onset_lag{lag}' for symptom in prob_b_columns for lag in lag_range] + \\\n",
    "                           [f'{symptom}_frequency_lag{lag}' for symptom in prob_b_columns for lag in lag_range] + \\\n",
    "                           [f'{symptom}_duration_lag{lag}' for symptom in prob_b_columns for lag in lag_range]\n",
    "\n",
    "# Calculate total number of symptoms (lagged)\n",
    "df['total_symptoms_lagged'] = df[lagged_symptom_features].sum(axis=1)\n",
    "\n",
    "# Additional feature engineering: Calculate magnitude of changes\n",
    "for col in outcome_columns:\n",
    "    df[f'magnitude_change_{col}'] = df[f'delta_' + col].abs()\n",
    "\n",
    "# Define feature sets\n",
    "binary_features = prob_b_columns\n",
    "numeric_features = [col for col in df.columns if col not in binary_features + ['actual_time_until_change', 'total_symptoms_lagged']]\n",
    "temporal_features = [f'{col}_frequency' for col in prob_b_columns] + \\\n",
    "                    [f'{col}_duration' for col in prob_b_columns] + \\\n",
    "                    [f'{col}_onset' for col in prob_b_columns] + \\\n",
    "                    [f'{col}_recurrence' for col in prob_b_columns]\n",
    "\n",
    "# Separate temporal features into lagged and current sets\n",
    "current_temporal_features = temporal_features\n",
    "\n",
    "# Final dataframe for modeling (do not dropna)\n",
    "df = df.reset_index(drop=True)\n",
    "# Check the structure of the dataframe\n",
    "df.isna().sum()\n",
    "\n",
    "# Plot the distribution of actual_time_until_change\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#df['actual_time_until_change'].dropna().plot(kind='hist', bins=range(int(df['actual_time_until_change'].min()), int(df['actual_time_until_change'].max()) + 2), alpha=0.7, color='blue')\n",
    "#plt.title('Distribution of Actual Time Until Change')\n",
    "#plt.xlabel('Time Until Change (months)')\n",
    "#plt.ylabel('Frequency')\n",
    "#plt.xticks(range(int(df['actual_time_until_change'].min()), int(df['actual_time_until_change'].max()) + 1))\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the end-of-life threshold (last 5 months)\n",
    "df['end_of_life_threshold'] = df['ddate'] - pd.DateOffset(months=5)\n",
    "\n",
    "\n",
    "# Remove the last 5 months of life for each participant who has died (Died == 1)\n",
    "df_no_eol = df[~((df['Died'] == 1) & (df['intdate'] >= df['end_of_life_threshold']))]\n",
    "\n",
    "df = df_no_eol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive modeling for TR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.1 (baseline using only previous TR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract features and target\n",
    "X = df[lagged_tr_features]\n",
    "y = df['TR']\n",
    "\n",
    "# Separate the binary and numeric features\n",
    "numeric_features = lagged_tr_features \n",
    "\n",
    "# Scale numeric features using QuantileTransformer\n",
    "scaler = QuantileTransformer(output_distribution='normal')\n",
    "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "# Train-test split\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "# Hyperparameter grid for RandomForestRegressor\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters:\\n{best_params}\")\n",
    "\n",
    "# Evaluate on training set\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "print(f\"Train MAE: {train_mae}\")\n",
    "print(f\"Train R-squared: {train_r2}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred_val = best_model.predict(X_val)\n",
    "val_mae = mean_absolute_error(y_val, y_pred_val)\n",
    "val_r2 = r2_score(y_val, y_pred_val)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "print(f\"Validation R-squared: {val_r2}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R-squared: {test_r2}\")\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(f\"Best hyperparameters:\\n{best_params}\")\n",
    "\n",
    "# Plot feature importances for the best model\n",
    "feature_importances = best_model.feature_importances_\n",
    "features = X_train.columns\n",
    "\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X_train.shape[1]), feature_importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), features[indices], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the model to a file \n",
    "with open('rfr_model_1_1.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "\n",
    "# Actual vs Predicted Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_test, c='blue', marker='o', label='Test data')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2)\n",
    "plt.xlabel('Actual values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('Actual vs Predicted Plot')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.2 (symptom-based model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract features and target\n",
    "X = df[lagged_tr_features + prob_b_columns + lagged_temporal_features]\n",
    "y = df['TR']\n",
    "\n",
    "# Separate the binary and numeric features\n",
    "numeric_features = lagged_tr_features \n",
    "\n",
    "# Scale numeric features using QuantileTransformer\n",
    "scaler = QuantileTransformer(output_distribution='normal')\n",
    "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "# Train-test split\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "# Hyperparameter grid for RandomForestRegressor\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters:\\n{best_params}\")\n",
    "\n",
    "# Evaluate on training set\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "print(f\"Train MAE: {train_mae}\")\n",
    "print(f\"Train R-squared: {train_r2}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred_val = best_model.predict(X_val)\n",
    "val_mae = mean_absolute_error(y_val, y_pred_val)\n",
    "val_r2 = r2_score(y_val, y_pred_val)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "print(f\"Validation R-squared: {val_r2}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R-squared: {test_r2}\")\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(f\"Best hyperparameters:\\n{best_params}\")\n",
    "\n",
    "# Plot feature importances for the best model\n",
    "feature_importances = best_model.feature_importances_\n",
    "features = X_train.columns\n",
    "\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X_train.shape[1]), feature_importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), features[indices], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the model to a file using pickle\n",
    "with open('rfr_model_1_2.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "\n",
    "# Actual vs Predicted Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_test, c='blue', marker='o', label='Test data')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2)\n",
    "plt.xlabel('Actual values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('Actual vs Predicted Plot')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.3 (top n features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract features and target\n",
    "X = df[lagged_tr_features + ['been fatigued (no energy/very tired)_frequency_lag1', 'been fatigued (no energy/very tired)_onset_lag1', \n",
    "                            'cold or flu symptoms_onset_lag1', 'been dizzy or unsteady on your feet_frequency_lag1']]\n",
    "y = df['TR']\n",
    "\n",
    "# Separate the binary and numeric features\n",
    "numeric_features = lagged_tr_features \n",
    "\n",
    "# Scale numeric features using QuantileTransformer\n",
    "scaler = QuantileTransformer(output_distribution='normal')\n",
    "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "# Train-test split\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "# Hyperparameter grid for RandomForestRegressor\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters:\\n{best_params}\")\n",
    "\n",
    "# Evaluate on training set\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "print(f\"Train MAE: {train_mae}\")\n",
    "print(f\"Train R-squared: {train_r2}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred_val = best_model.predict(X_val)\n",
    "val_mae = mean_absolute_error(y_val, y_pred_val)\n",
    "val_r2 = r2_score(y_val, y_pred_val)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "print(f\"Validation R-squared: {val_r2}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R-squared: {test_r2}\")\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(f\"Best hyperparameters:\\n{best_params}\")\n",
    "\n",
    "# Plot feature importances for the best model\n",
    "feature_importances = best_model.feature_importances_\n",
    "features = X_train.columns\n",
    "\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X_train.shape[1]), feature_importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), features[indices], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the model to a file using pickle\n",
    "with open('rfr_model_1_3.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "\n",
    "# Actual vs Predicted Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_test, c='blue', marker='o', label='Test data')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2)\n",
    "plt.xlabel('Actual values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('Actual vs Predicted Plot')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive modeling for Symptom reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2.1 (symptom-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[lagged_symptom_features + lagged_tr_features + ['TR'] + lagged_temporal_features]\n",
    "y = df[prob_b_columns]\n",
    "\n",
    "# Separate the binary and numeric features\n",
    "numeric_features = lagged_tr_features + ['TR']\n",
    "\n",
    "# Scale numeric features using QuantileTransformer\n",
    "scaler = QuantileTransformer(output_distribution='normal')\n",
    "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "# Check shapes of X and y to ensure they are consistent\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# Ensure the indices of X and y match\n",
    "X, y = X.align(y, join='inner', axis=0)\n",
    "print(\"Shape of X after alignment:\", X.shape)\n",
    "print(\"Shape of y after alignment:\", y.shape)\n",
    "\n",
    "# Train-test split\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "# Hyperparameter grid for RandomForestClassifier\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [50, 100, 200],\n",
    "    'estimator__max_depth': [10, 20, None],\n",
    "    'estimator__min_samples_split': [2, 5, 10],\n",
    "    'estimator__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "base_model = RandomForestClassifier(random_state=42)\n",
    "model = MultiOutputClassifier(base_model)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters:\\n{best_params}\")\n",
    "\n",
    "# Evaluate on training set\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='macro')\n",
    "print(f\"Train F1: {train_f1}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred_val = best_model.predict(X_val)\n",
    "val_f1 = f1_score(y_val, y_pred_val, average='macro')\n",
    "print(f\"Validation F1: {val_f1}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "print(f\"Test F1: {test_f1}\")\n",
    "\n",
    "# Calculate PR AUC for test set\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "pr_auc_list = []\n",
    "\n",
    "y_pred_prob_test = np.stack([best_model.estimators_[i].predict_proba(X_test)[:, 1] for i in range(len(prob_b_columns))], axis=1)\n",
    "for i, col in enumerate(prob_b_columns):\n",
    "    precision, recall, _ = precision_recall_curve(y_test.iloc[:, i], y_pred_prob_test[:, i])\n",
    "    pr_auc = auc(recall, precision)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    pr_auc_list.append(pr_auc)\n",
    "    plt.plot(recall, precision, lw=2, label=f'{col} (AUC = {pr_auc:.2f})')\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Average Train F1:\", train_f1)\n",
    "print(\"Average Validation F1:\", val_f1)\n",
    "print(\"Average Test F1:\", test_f1)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(f\"Best hyperparameters:\\n{best_params}\")\n",
    "\n",
    "# Save the model to a file using pickle\n",
    "with open('rfc_model_2_1.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('PR Curve for test set')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2.2 (symptom-disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[lagged_symptom_features + lagged_tr_features + ['TR'] + lagged_temporal_features + disease_columns + lagged_outcomes]\n",
    "y = df[prob_b_columns]\n",
    "\n",
    "# Separate the binary and numeric features\n",
    "numeric_features = lagged_tr_features + ['TR']\n",
    "\n",
    "# Scale numeric features using QuantileTransformer\n",
    "scaler = QuantileTransformer(output_distribution='normal')\n",
    "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "# Check shapes of X and y to ensure they are consistent\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# Ensure the indices of X and y match\n",
    "X, y = X.align(y, join='inner', axis=0)\n",
    "print(\"Shape of X after alignment:\", X.shape)\n",
    "print(\"Shape of y after alignment:\", y.shape)\n",
    "\n",
    "# Train-test split\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "# Hyperparameter grid for RandomForestClassifier\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [50, 100, 200],\n",
    "    'estimator__max_depth': [10, 20, None],\n",
    "    'estimator__min_samples_split': [2, 5, 10],\n",
    "    'estimator__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "base_model = RandomForestClassifier(random_state=42)\n",
    "model = MultiOutputClassifier(base_model)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters:\\n{best_params}\")\n",
    "\n",
    "# Evaluate on training set\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='macro')\n",
    "print(f\"Train F1: {train_f1}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred_val = best_model.predict(X_val)\n",
    "val_f1 = f1_score(y_val, y_pred_val, average='macro')\n",
    "print(f\"Validation F1: {val_f1}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "print(f\"Test F1: {test_f1}\")\n",
    "\n",
    "# Calculate PR AUC for test set\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "pr_auc_list = []\n",
    "\n",
    "y_pred_prob_test = np.stack([best_model.estimators_[i].predict_proba(X_test)[:, 1] for i in range(len(prob_b_columns))], axis=1)\n",
    "for i, col in enumerate(prob_b_columns):\n",
    "    precision, recall, _ = precision_recall_curve(y_test.iloc[:, i], y_pred_prob_test[:, i])\n",
    "    pr_auc = auc(recall, precision)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    pr_auc_list.append(pr_auc)\n",
    "    plt.plot(recall, precision, lw=2, label=f'{col} (AUC = {pr_auc:.2f})')\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Average Train F1:\", train_f1)\n",
    "print(\"Average Validation F1:\", val_f1)\n",
    "print(\"Average Test F1:\", test_f1)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(f\"Best hyperparameters:\\n{best_params}\")\n",
    "\n",
    "# Save the model to a file using pickle\n",
    "with open('rfc_model_2_2.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('PR Curve for test set')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['pain or stiffness in your joints_duration_lag1', 'pain or stiffness in your joints_onset_lag1', \n",
    "        'pain or stiffness in your joints_frequency_lag1', 'pain or stiffness in your back_frequency_lag1',\n",
    "        'been fatigued (no energy/very tired)_frequency_lag1']]\n",
    "y = df[prob_b_columns]\n",
    "\n",
    "# Separate the binary and numeric features\n",
    "#numeric_features = lagged_tr_features + ['TR']\n",
    "\n",
    "# Scale numeric features using QuantileTransformer\n",
    "#scaler = QuantileTransformer(output_distribution='normal')\n",
    "#X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "# Check shapes of X and y to ensure they are consistent\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# Ensure the indices of X and y match\n",
    "X, y = X.align(y, join='inner', axis=0)\n",
    "print(\"Shape of X after alignment:\", X.shape)\n",
    "print(\"Shape of y after alignment:\", y.shape)\n",
    "\n",
    "# Train-test split\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "# Hyperparameter grid for RandomForestClassifier\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [50, 100, 200],\n",
    "    'estimator__max_depth': [10, 20, None],\n",
    "    'estimator__min_samples_split': [2, 5, 10],\n",
    "    'estimator__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "base_model = RandomForestClassifier(random_state=42)\n",
    "model = MultiOutputClassifier(base_model)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters:\\n{best_params}\")\n",
    "\n",
    "# Evaluate on training set\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='macro')\n",
    "print(f\"Train F1: {train_f1}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred_val = best_model.predict(X_val)\n",
    "val_f1 = f1_score(y_val, y_pred_val, average='macro')\n",
    "print(f\"Validation F1: {val_f1}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "print(f\"Test F1: {test_f1}\")\n",
    "\n",
    "# Calculate PR AUC for test set\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "pr_auc_list = []\n",
    "\n",
    "y_pred_prob_test = np.stack([best_model.estimators_[i].predict_proba(X_test)[:, 1] for i in range(len(prob_b_columns))], axis=1)\n",
    "for i, col in enumerate(prob_b_columns):\n",
    "    precision, recall, _ = precision_recall_curve(y_test.iloc[:, i], y_pred_prob_test[:, i])\n",
    "    pr_auc = auc(recall, precision)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    pr_auc_list.append(pr_auc)\n",
    "    plt.plot(recall, precision, lw=2, label=f'{col} (AUC = {pr_auc:.2f})')\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Average Train F1:\", train_f1)\n",
    "print(\"Average Validation F1:\", val_f1)\n",
    "print(\"Average Test F1:\", test_f1)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(f\"Best hyperparameters:\\n{best_params}\")\n",
    "\n",
    "# Save the model to a file using pickle\n",
    "with open('rfc_model_2_3.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('PR Curve for test set')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive modeling for TUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['event_observed'] = df['combined_change'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of people who never have event_observed\n",
    "never_event_observed = df.groupby('studyID')['event_observed'].sum() == 0\n",
    "num_never_event_observed = never_event_observed.sum()\n",
    "print(f\"Number of people who never have event_observed: {num_never_event_observed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3.1 (symptom-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract features and target\n",
    "X = df[prob_b_columns + lagged_tr_features + lagged_temporal_features]\n",
    "y = df['actual_time_until_change']\n",
    "\n",
    "# Separate the binary and numeric features\n",
    "numeric_features = lagged_tr_features\n",
    "\n",
    "# Scale numeric features using QuantileTransformer\n",
    "scaler = QuantileTransformer(output_distribution='normal')\n",
    "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "# Manually split the data into training and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "# Hyperparameter grid for RandomForestRegressor\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning on the training set with 5-fold cross-validation\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters:\\n{best_params}\")\n",
    "\n",
    "# Evaluate on training set\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "print(f\"Train MAE: {train_mae}\")\n",
    "print(f\"Train R-squared: {train_r2}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred_val = best_model.predict(X_val)\n",
    "val_mae = mean_absolute_error(y_val, y_pred_val)\n",
    "val_r2 = r2_score(y_val, y_pred_val)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "print(f\"Validation R-squared: {val_r2}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R-squared: {test_r2}\")\n",
    "\n",
    "\n",
    "# Save the model to a file using pickle\n",
    "with open('rfr_model_3_1.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "\n",
    "# Actual vs Predicted Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_test, c='blue', marker='o', label='Test data')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2)\n",
    "plt.xlabel('Actual values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('Actual vs Predicted Plot')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3.2 (symptom-disease-outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract features and target\n",
    "X = df[prob_b_columns + lagged_tr_features + lagged_temporal_features + disease_columns + lagged_outcomes]\n",
    "y = df['actual_time_until_change']\n",
    "\n",
    "# Separate the binary and numeric features\n",
    "numeric_features = lagged_tr_features\n",
    "\n",
    "# Scale numeric features using QuantileTransformer\n",
    "scaler = QuantileTransformer(output_distribution='normal')\n",
    "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "# Manually split the data into training and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "# Hyperparameter grid for RandomForestRegressor\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning on the training set with 5-fold cross-validation\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters:\\n{best_params}\")\n",
    "\n",
    "# Evaluate on training set\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "print(f\"Train MAE: {train_mae}\")\n",
    "print(f\"Train R-squared: {train_r2}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred_val = best_model.predict(X_val)\n",
    "val_mae = mean_absolute_error(y_val, y_pred_val)\n",
    "val_r2 = r2_score(y_val, y_pred_val)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "print(f\"Validation R-squared: {val_r2}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R-squared: {test_r2}\")\n",
    "\n",
    "\n",
    "# Save the model to a file using pickle\n",
    "with open('rfr_model_3_2.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "\n",
    "# Actual vs Predicted Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_test, c='blue', marker='o', label='Test data')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2)\n",
    "plt.xlabel('Actual values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('Actual vs Predicted Plot')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3.3 (top n features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract features and target\n",
    "X = df[lagged_tr_features + ['been fatigued (no energy/very tired)_onset_lag1', 'been fatigued (no energy/very tired)_frequency_lag1', \n",
    "                            'nausea, vomiting, diarrhea, or other stomach (abdominal) problem_frequency_lag1', \n",
    "                             'pain or stiffness in your joints_frequency_lag1']]\n",
    "y = df['actual_time_until_change']\n",
    "\n",
    "# Separate the binary and numeric features\n",
    "numeric_features = lagged_tr_features\n",
    "\n",
    "# Scale numeric features using QuantileTransformer\n",
    "scaler = QuantileTransformer(output_distribution='normal')\n",
    "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "# Manually split the data into training and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "# Hyperparameter grid for RandomForestRegressor\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning on the training set with 5-fold cross-validation\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters:\\n{best_params}\")\n",
    "\n",
    "# Evaluate on training set\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "print(f\"Train MAE: {train_mae}\")\n",
    "print(f\"Train R-squared: {train_r2}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred_val = best_model.predict(X_val)\n",
    "val_mae = mean_absolute_error(y_val, y_pred_val)\n",
    "val_r2 = r2_score(y_val, y_pred_val)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "print(f\"Validation R-squared: {val_r2}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R-squared: {test_r2}\")\n",
    "\n",
    "\n",
    "# Save the model to a file using pickle\n",
    "with open('rfr_model_3_3.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "\n",
    "# Actual vs Predicted Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_test, c='blue', marker='o', label='Test data')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2)\n",
    "plt.xlabel('Actual values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('Actual vs Predicted Plot')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Author: Nikhil Kashyap, MSc Information Studies: Data Science, UvA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
